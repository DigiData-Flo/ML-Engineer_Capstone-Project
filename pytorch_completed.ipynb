{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65e9dc4",
   "metadata": {},
   "source": [
    "# ML Model with pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ddb93de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb0a90",
   "metadata": {},
   "source": [
    "## Import the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f307e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('data/features_completed.csv', index_col=0)\n",
    "labels = pd.read_csv('data/labels_completed.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a4b8ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63288, 16), (63288, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c32b575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>reward</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>duration</th>\n",
       "      <th>email</th>\n",
       "      <th>mobile</th>\n",
       "      <th>social</th>\n",
       "      <th>web</th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "      <th>O</th>\n",
       "      <th>U</th>\n",
       "      <th>bogo</th>\n",
       "      <th>discount</th>\n",
       "      <th>informational</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.180723</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.072289</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.445783</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63283</th>\n",
       "      <td>0.438476</td>\n",
       "      <td>0.393389</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63284</th>\n",
       "      <td>0.438476</td>\n",
       "      <td>0.393389</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63285</th>\n",
       "      <td>0.438476</td>\n",
       "      <td>0.393389</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63286</th>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63287</th>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63288 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    income  reward  difficulty  duration  email  mobile  \\\n",
       "0      0.180723  0.466667       5           5       120      1       1   \n",
       "1      0.072289  0.333333       5           5       120      1       1   \n",
       "2      0.445783  0.488889       5           5       120      1       1   \n",
       "3      0.433735  0.766667       5           5       120      1       1   \n",
       "4      0.530120  0.566667       5           5       120      1       1   \n",
       "...         ...       ...     ...         ...       ...    ...     ...   \n",
       "63283  0.438476  0.393389      10          10       168      1       1   \n",
       "63284  0.438476  0.393389      10          10       168      1       1   \n",
       "63285  0.438476  0.393389      10          10       168      1       1   \n",
       "63286  0.759036  0.388889      10          10       168      1       1   \n",
       "63287  0.024096  0.155556      10          10       168      1       1   \n",
       "\n",
       "       social  web  F  M  O  U  bogo  discount  informational  \n",
       "0           1    1  0  1  0  0     1         0              0  \n",
       "1           1    1  1  0  0  0     1         0              0  \n",
       "2           1    1  1  0  0  0     1         0              0  \n",
       "3           1    1  0  1  0  0     1         0              0  \n",
       "4           1    1  1  0  0  0     1         0              0  \n",
       "...       ...  ... .. .. .. ..   ...       ...            ...  \n",
       "63283       1    0  0  0  0  1     1         0              0  \n",
       "63284       1    0  0  0  0  1     1         0              0  \n",
       "63285       1    0  0  0  0  1     1         0              0  \n",
       "63286       1    0  1  0  0  0     1         0              0  \n",
       "63287       1    0  0  1  0  0     1         0              0  \n",
       "\n",
       "[63288 rows x 16 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb6e82",
   "metadata": {},
   "source": [
    "## Create Training, Validation and Testdata\n",
    "To avoid overfitting I split the train data additional in validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "619b6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e79d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the dataset into 2/3 training and 1/3 testing sets.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, targets, test_size=0.33)\n",
    "\n",
    "# Then we split the training set further into 2/3 training and 1/3 validation sets.\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "996ca774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28409, 16), (13993, 16))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af208051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory and make sure that the directory exists\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee9f04",
   "metadata": {},
   "source": [
    "## Create csv files for test, validation and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c706ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>binary_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29285</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32350</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34646</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39901</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44106</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11231</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31283</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62075</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20886 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       binary_target\n",
       "29285              1\n",
       "32350              1\n",
       "34646              1\n",
       "2864               0\n",
       "39901              1\n",
       "...              ...\n",
       "44106              0\n",
       "11231              1\n",
       "31283              0\n",
       "62075              1\n",
       "6990               1\n",
       "\n",
       "[20886 rows x 1 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b849e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use pandas to save our test, train and validation data to csv files. Note that we make sure not to include header\n",
    "# information or an index as this is required by the built in algorithms provided by Amazon. Also, for the train and\n",
    "# validation data, it is assumed that the first entry in each row is the target variable.\n",
    "\n",
    "pd.concat([Y_test, X_test], axis=1).to_csv(os.path.join(data_dir, 'test_completed_torch.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([Y_val, X_val], axis=1).to_csv(os.path.join(data_dir, 'validation_completed_torch.csv'), header=False, index=False)\n",
    "pd.concat([Y_train, X_train], axis=1).to_csv(os.path.join(data_dir, 'train_completed_torch.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc28de",
   "metadata": {},
   "source": [
    "### Create a PyTorch Estimator\n",
    "\n",
    "You've had some practice instantiating built-in models in SageMaker. All estimators require some constructor arguments to be passed in. When a custom model is constructed in SageMaker, an **entry point** must be specified. The entry_point is the training script that will be executed when the model is trained; the `train.py` function you specified above! \n",
    "\n",
    "#### Model size\n",
    " I found an interisting article how to choose the number and size of hidden layers [choosing-number-hidden-layers-neurons-neural-networks](https://www.linkedin.com/pulse/choosing-number-hidden-layers-neurons-neural-networks-sachdev/)\n",
    "\n",
    "#### Instance Types\n",
    "\n",
    "It is suggested that you use instances that are available in the free tier of usage: `'ml.c4.xlarge'` for training and `'ml.t2.medium'` for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa229895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input features\n",
    "input_dim = features.shape[1]\n",
    "# choose 3 hidden layers for input features count\n",
    "# Start with 8 neurons for layer 1\n",
    "hidden_1 = 8\n",
    "hidden_2 = 4\n",
    "hidden_3 = 2\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3c93de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 8, 4, 2, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim, hidden_1, hidden_2, hidden_3, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "266439a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to evaluate the endpoint on test data\n",
    "# returns a variety of model metrics\n",
    "def evaluate(predictor, test_features, test_labels, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a test set given the prediction endpoint.  \n",
    "    Return binary classification metrics.\n",
    "    :param predictor: A prediction endpoint\n",
    "    :param test_features: Test features\n",
    "    :param test_labels: Class labels for test data\n",
    "    :param verbose: If True, prints a table of all performance metrics\n",
    "    :return: A dictionary of performance metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # rounding and squeezing array\n",
    "    test_preds = np.squeeze(np.round(predictor.predict(test_features)))\n",
    "    \n",
    "    # calculate true positives, false positives, true negatives, false negatives\n",
    "    tp = np.logical_and(test_labels, test_preds).sum()\n",
    "    fp = np.logical_and(1-test_labels, test_preds).sum()\n",
    "    tn = np.logical_and(1-test_labels, 1-test_preds).sum()\n",
    "    fn = np.logical_and(test_labels, 1-test_preds).sum()\n",
    "    \n",
    "    # calculate binary classification metrics\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    \n",
    "    # print metrics\n",
    "    if verbose:\n",
    "        print(pd.crosstab(test_labels, test_preds, rownames=['actuals'], colnames=['predictions']))\n",
    "        print(\"\\n{:<11} {:.3f}\".format('Recall:', recall))\n",
    "        print(\"{:<11} {:.3f}\".format('Precision:', precision))\n",
    "        print(\"{:<11} {:.3f}\".format('Accuracy:', accuracy))\n",
    "        print()\n",
    "        \n",
    "    return {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn, \n",
    "            'Precision': precision, 'Recall': recall, 'Accuracy': accuracy}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32c4d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## TODO: Complete this classifier\n",
    "class SimpleNet(nn.Module):\n",
    "    \n",
    "    ## TODO: Define the init function\n",
    "    def __init__(self, input_dim, hidden_1, hidden_2, hidden_3, output_dim):\n",
    "        '''Defines layers of a neural network.\n",
    "           :param input_dim: Number of input features\n",
    "           :param hidden_dim: Size of hidden layer(s)\n",
    "           :param output_dim: Number of outputs\n",
    "         '''\n",
    "        super(SimpleNet, self).__init__()\n",
    "        \n",
    "        # define all layers, here\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.fc4 = nn.Linear(hidden_3, output_dim)\n",
    "        # Define dropout\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        # Sigmoid Layer\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    ## TODO: Define the feedforward behavior of the network\n",
    "    def forward(self, x):\n",
    "        '''Feedforward behavior of the net.\n",
    "           :param x: A batch of input features\n",
    "           :return: A single, sigmoid activated value\n",
    "         '''\n",
    "        # your code, here\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.drop(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.drop(out)\n",
    "        out = F.relu(self.fc3(out))\n",
    "        out = self.drop(out)\n",
    "        out = self.fc4(out)\n",
    "      \n",
    "        return self.sig(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438e560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "acfb2a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (fc1): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (fc2): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (fc3): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (fc4): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleNet(input_dim, hidden_1, hidden_2, hidden_3, output_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "347a71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data from a csv file\n",
    "def _get_train_loader(batch_size, data_dir, file):\n",
    "    print(\"Get train loader.\")\n",
    "\n",
    "    # read in csv file\n",
    "    train_data = pd.read_csv(os.path.join(data_dir, file), header=None, names=None)\n",
    "\n",
    "    # labels are first column\n",
    "    train_y = torch.from_numpy(train_data[[0]].values).float()\n",
    "    # features are the rest\n",
    "    train_x = torch.from_numpy(train_data.drop([0], axis=1).values).float()\n",
    "\n",
    "    # create dataset\n",
    "    train_ds = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "\n",
    "    return torch.utils.data.DataLoader(train_ds, batch_size=batch_size)\n",
    "\n",
    "# Load the training data from a csv file\n",
    "def _get_validation_loader(batch_size, data_dir, file):\n",
    "    print(\"Get validation loader.\")\n",
    "\n",
    "    # read in csv file\n",
    "    val_data = pd.read_csv(os.path.join(data_dir, file), header=None, names=None)\n",
    "\n",
    "    # labels are first column\n",
    "    val_y = torch.from_numpy(val_data[[0]].values).float()\n",
    "    # features are the rest\n",
    "    val_x = torch.from_numpy(val_data.drop([0], axis=1).values).float()\n",
    "\n",
    "    # create dataset\n",
    "    val_ds = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "\n",
    "    return torch.utils.data.DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "# Load the training data from a csv file\n",
    "def _get_test_loader(batch_size, data_dir, file):\n",
    "    print(\"Get test loader.\")\n",
    "\n",
    "    # read in csv file\n",
    "    test_data = pd.read_csv(os.path.join(data_dir, file), header=None, names=None)\n",
    "\n",
    "    # labels are first column\n",
    "    test_y = torch.from_numpy(test_data[[0]].values).float()\n",
    "    # features are the rest\n",
    "    test_x = torch.from_numpy(test_data.drop([0], axis=1).values).float()\n",
    "\n",
    "    # create dataset\n",
    "    test_ds = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "\n",
    "    return torch.utils.data.DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Provided train function\n",
    "def train(model, train_loader, validation_loader, epochs, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    This is the training method that is called by the PyTorch training script. The parameters\n",
    "    passed are as follows:\n",
    "    model        - The PyTorch model that we wish to train.\n",
    "    train_loader - The PyTorch DataLoader that should be used during training.\n",
    "    epochs       - The total number of epochs to train for.\n",
    "    optimizer    - The optimizer to use during training.\n",
    "    criterion    - The loss function used for training. \n",
    "    device       - Where the model and data should be loaded (gpu or cpu).\n",
    "    \"\"\"\n",
    "    valid_loss_min = np.Inf # track change in validation loss\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        valid_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "            # prep data\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad() # zero accumulated gradients\n",
    "            # get output of SimpleNet\n",
    "            output = model(data)\n",
    "            # calculate loss and perform backprop\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    " \n",
    "        for batch_idx, (data, target) in enumerate(validation_loader, 1):\n",
    "            # prep data\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # get output of SimpleNet\n",
    "            output = model(data)\n",
    "            # calculate loss and perform backprop\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # print loss stats\n",
    "        print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss / len(train_loader)))\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, total_loss / len(train_loader), valid_loss / len(validation_loader)))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  \\\n",
    "                   Saving model ...'.format(valid_loss_min/len(validation_loader), valid_loss/len(validation_loader)))\n",
    "            torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "            # Set model back to device after saving\n",
    "            model.to(device)\n",
    "            valid_loss_min = valid_loss        \n",
    "        \n",
    "    # save after all epochs\n",
    "    save_model(model, model_dir)\n",
    "\n",
    "\n",
    "# Provided model saving functions\n",
    "def save_model(model, model_dir):\n",
    "    print(\"Saving the model.\")\n",
    "    path = os.path.join(model_dir, 'model.pth')\n",
    "    # save state dictionary\n",
    "    torch.save(model.cpu().state_dict(), path)\n",
    "    \n",
    "def save_model_params(model, model_dir):\n",
    "    model_info_path = os.path.join(args.model_dir, 'model_info.pth')\n",
    "    with open(model_info_path, 'wb') as f:\n",
    "        model_info = {\n",
    "            'input_dim': args.input_dim,\n",
    "            'hidden_1': args.hidden_1,\n",
    "            'hidden_2': args.hidden_2,\n",
    "            'hidden_3': args.hidden_3,\n",
    "            'output_dim': args.output_dim\n",
    "        }\n",
    "        torch.save(model_info, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6a303f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get train loader.\n",
      "Get validation loader.\n",
      "Get test loader.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "model_dir = 'data'\n",
    "train_loader = _get_train_loader(batch_size=64, data_dir=data_dir, file='train_completed_torch.csv')\n",
    "validation_loader = _get_validation_loader(batch_size=64, data_dir=data_dir, file='validation_completed_torch.csv')\n",
    "test_loader = _get_test_loader(batch_size=64, data_dir=data_dir, file='test_completed_torch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "37c7ed06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def2401a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6185850b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6905444115400314\n",
      "Epoch: 1 \tTraining Loss: 0.690544 \tValidation Loss: 42.688085\n",
      "Validation loss decreased (inf --> 42.688085).                     Saving model ...\n",
      "Epoch: 2, Loss: 0.665485772195163\n",
      "Epoch: 2 \tTraining Loss: 0.665486 \tValidation Loss: 41.989956\n",
      "Validation loss decreased (42.688085 --> 41.989956).                     Saving model ...\n",
      "Epoch: 3, Loss: 0.6574403902700355\n",
      "Epoch: 3 \tTraining Loss: 0.657440 \tValidation Loss: 41.389905\n",
      "Validation loss decreased (41.989956 --> 41.389905).                     Saving model ...\n",
      "Epoch: 4, Loss: 0.6500213009280127\n",
      "Epoch: 4 \tTraining Loss: 0.650021 \tValidation Loss: 40.930405\n",
      "Validation loss decreased (41.389905 --> 40.930405).                     Saving model ...\n",
      "Epoch: 5, Loss: 0.6379747110175656\n",
      "Epoch: 5 \tTraining Loss: 0.637975 \tValidation Loss: 39.683065\n",
      "Validation loss decreased (40.930405 --> 39.683065).                     Saving model ...\n",
      "Epoch: 6, Loss: 0.6280063281993609\n",
      "Epoch: 6 \tTraining Loss: 0.628006 \tValidation Loss: 39.533851\n",
      "Validation loss decreased (39.683065 --> 39.533851).                     Saving model ...\n",
      "Epoch: 7, Loss: 0.6235953225745811\n",
      "Epoch: 7 \tTraining Loss: 0.623595 \tValidation Loss: 39.048427\n",
      "Validation loss decreased (39.533851 --> 39.048427).                     Saving model ...\n",
      "Epoch: 8, Loss: 0.6211206436425716\n",
      "Epoch: 8 \tTraining Loss: 0.621121 \tValidation Loss: 39.053883\n",
      "Epoch: 9, Loss: 0.6179728592569763\n",
      "Epoch: 9 \tTraining Loss: 0.617973 \tValidation Loss: 38.871577\n",
      "Validation loss decreased (39.048427 --> 38.871577).                     Saving model ...\n",
      "Epoch: 10, Loss: 0.6178275788421029\n",
      "Epoch: 10 \tTraining Loss: 0.617828 \tValidation Loss: 38.706309\n",
      "Validation loss decreased (38.871577 --> 38.706309).                     Saving model ...\n",
      "Epoch: 11, Loss: 0.6159219176651122\n",
      "Epoch: 11 \tTraining Loss: 0.615922 \tValidation Loss: 38.509794\n",
      "Validation loss decreased (38.706309 --> 38.509794).                     Saving model ...\n",
      "Epoch: 12, Loss: 0.6151771905857164\n",
      "Epoch: 12 \tTraining Loss: 0.615177 \tValidation Loss: 38.774664\n",
      "Epoch: 13, Loss: 0.6145356864005596\n",
      "Epoch: 13 \tTraining Loss: 0.614536 \tValidation Loss: 39.088518\n",
      "Epoch: 14, Loss: 0.6146851153792562\n",
      "Epoch: 14 \tTraining Loss: 0.614685 \tValidation Loss: 38.731449\n",
      "Epoch: 15, Loss: 0.6140474068957407\n",
      "Epoch: 15 \tTraining Loss: 0.614047 \tValidation Loss: 39.188125\n",
      "Epoch: 16, Loss: 0.6150337518066973\n",
      "Epoch: 16 \tTraining Loss: 0.615034 \tValidation Loss: 38.514420\n",
      "Epoch: 17, Loss: 0.6141800973732192\n",
      "Epoch: 17 \tTraining Loss: 0.614180 \tValidation Loss: 38.638315\n",
      "Epoch: 18, Loss: 0.6139673620193928\n",
      "Epoch: 18 \tTraining Loss: 0.613967 \tValidation Loss: 38.880109\n",
      "Epoch: 19, Loss: 0.6135615166108888\n",
      "Epoch: 19 \tTraining Loss: 0.613562 \tValidation Loss: 38.840118\n",
      "Epoch: 20, Loss: 0.6156801567287058\n",
      "Epoch: 20 \tTraining Loss: 0.615680 \tValidation Loss: 38.776137\n",
      "Epoch: 21, Loss: 0.6151458992882892\n",
      "Epoch: 21 \tTraining Loss: 0.615146 \tValidation Loss: 38.483271\n",
      "Validation loss decreased (38.509794 --> 38.483271).                     Saving model ...\n",
      "Epoch: 22, Loss: 0.6133509268900296\n",
      "Epoch: 22 \tTraining Loss: 0.613351 \tValidation Loss: 38.650491\n",
      "Epoch: 23, Loss: 0.6129089215988511\n",
      "Epoch: 23 \tTraining Loss: 0.612909 \tValidation Loss: 38.725958\n",
      "Epoch: 24, Loss: 0.6131268310788516\n",
      "Epoch: 24 \tTraining Loss: 0.613127 \tValidation Loss: 38.522933\n",
      "Epoch: 25, Loss: 0.6109655457171234\n",
      "Epoch: 25 \tTraining Loss: 0.610966 \tValidation Loss: 38.373698\n",
      "Validation loss decreased (38.483271 --> 38.373698).                     Saving model ...\n",
      "Epoch: 26, Loss: 0.6137383237212628\n",
      "Epoch: 26 \tTraining Loss: 0.613738 \tValidation Loss: 38.748723\n",
      "Epoch: 27, Loss: 0.6121815065811346\n",
      "Epoch: 27 \tTraining Loss: 0.612182 \tValidation Loss: 38.465548\n",
      "Epoch: 28, Loss: 0.6143044453483444\n",
      "Epoch: 28 \tTraining Loss: 0.614304 \tValidation Loss: 38.762953\n",
      "Epoch: 29, Loss: 0.6125827575320596\n",
      "Epoch: 29 \tTraining Loss: 0.612583 \tValidation Loss: 38.785223\n",
      "Epoch: 30, Loss: 0.6128063175055358\n",
      "Epoch: 30 \tTraining Loss: 0.612806 \tValidation Loss: 39.283971\n",
      "Epoch: 31, Loss: 0.613444280718361\n",
      "Epoch: 31 \tTraining Loss: 0.613444 \tValidation Loss: 38.496998\n",
      "Epoch: 32, Loss: 0.6109504754747357\n",
      "Epoch: 32 \tTraining Loss: 0.610950 \tValidation Loss: 38.705219\n",
      "Epoch: 33, Loss: 0.6108423596969595\n",
      "Epoch: 33 \tTraining Loss: 0.610842 \tValidation Loss: 38.938582\n",
      "Epoch: 34, Loss: 0.6096386695364574\n",
      "Epoch: 34 \tTraining Loss: 0.609639 \tValidation Loss: 38.797025\n",
      "Epoch: 35, Loss: 0.611526565978656\n",
      "Epoch: 35 \tTraining Loss: 0.611527 \tValidation Loss: 38.739406\n",
      "Epoch: 36, Loss: 0.6120821334220268\n",
      "Epoch: 36 \tTraining Loss: 0.612082 \tValidation Loss: 38.985984\n",
      "Epoch: 37, Loss: 0.6125691379378507\n",
      "Epoch: 37 \tTraining Loss: 0.612569 \tValidation Loss: 38.550137\n",
      "Epoch: 38, Loss: 0.6106107970615765\n",
      "Epoch: 38 \tTraining Loss: 0.610611 \tValidation Loss: 38.483663\n",
      "Epoch: 39, Loss: 0.6110454372189067\n",
      "Epoch: 39 \tTraining Loss: 0.611045 \tValidation Loss: 38.732189\n",
      "Epoch: 40, Loss: 0.6104527211806796\n",
      "Epoch: 40 \tTraining Loss: 0.610453 \tValidation Loss: 38.849695\n",
      "Epoch: 41, Loss: 0.6102704697095596\n",
      "Epoch: 41 \tTraining Loss: 0.610270 \tValidation Loss: 38.388003\n",
      "Epoch: 42, Loss: 0.6097147412396766\n",
      "Epoch: 42 \tTraining Loss: 0.609715 \tValidation Loss: 39.086046\n",
      "Epoch: 43, Loss: 0.6094677604667775\n",
      "Epoch: 43 \tTraining Loss: 0.609468 \tValidation Loss: 38.611027\n",
      "Epoch: 44, Loss: 0.6113948234029718\n",
      "Epoch: 44 \tTraining Loss: 0.611395 \tValidation Loss: 38.306067\n",
      "Validation loss decreased (38.373698 --> 38.306067).                     Saving model ...\n",
      "Epoch: 45, Loss: 0.6111429966650568\n",
      "Epoch: 45 \tTraining Loss: 0.611143 \tValidation Loss: 38.407508\n",
      "Epoch: 46, Loss: 0.6098269561792279\n",
      "Epoch: 46 \tTraining Loss: 0.609827 \tValidation Loss: 38.573226\n",
      "Epoch: 47, Loss: 0.6082847512385867\n",
      "Epoch: 47 \tTraining Loss: 0.608285 \tValidation Loss: 38.306415\n",
      "Epoch: 48, Loss: 0.6085125169641262\n",
      "Epoch: 48 \tTraining Loss: 0.608513 \tValidation Loss: 39.043697\n",
      "Epoch: 49, Loss: 0.6093746617704898\n",
      "Epoch: 49 \tTraining Loss: 0.609375 \tValidation Loss: 38.480805\n",
      "Epoch: 50, Loss: 0.6092967828651806\n",
      "Epoch: 50 \tTraining Loss: 0.609297 \tValidation Loss: 38.776215\n",
      "Epoch: 51, Loss: 0.6088070848637873\n",
      "Epoch: 51 \tTraining Loss: 0.608807 \tValidation Loss: 38.589150\n",
      "Epoch: 52, Loss: 0.6118808853867892\n",
      "Epoch: 52 \tTraining Loss: 0.611881 \tValidation Loss: 38.329490\n",
      "Epoch: 53, Loss: 0.6098599898385572\n",
      "Epoch: 53 \tTraining Loss: 0.609860 \tValidation Loss: 38.406092\n",
      "Epoch: 54, Loss: 0.6091251156485833\n",
      "Epoch: 54 \tTraining Loss: 0.609125 \tValidation Loss: 38.542167\n",
      "Epoch: 55, Loss: 0.6096194179074185\n",
      "Epoch: 55 \tTraining Loss: 0.609619 \tValidation Loss: 38.567801\n",
      "Epoch: 56, Loss: 0.609938841950786\n",
      "Epoch: 56 \tTraining Loss: 0.609939 \tValidation Loss: 38.650885\n",
      "Epoch: 57, Loss: 0.6117974164652394\n",
      "Epoch: 57 \tTraining Loss: 0.611797 \tValidation Loss: 38.490911\n",
      "Epoch: 58, Loss: 0.6076533155398326\n",
      "Epoch: 58 \tTraining Loss: 0.607653 \tValidation Loss: 38.487176\n",
      "Epoch: 59, Loss: 0.6077449778178791\n",
      "Epoch: 59 \tTraining Loss: 0.607745 \tValidation Loss: 38.622021\n",
      "Epoch: 60, Loss: 0.6113689580747673\n",
      "Epoch: 60 \tTraining Loss: 0.611369 \tValidation Loss: 39.094077\n",
      "Epoch: 61, Loss: 0.6098087959595628\n",
      "Epoch: 61 \tTraining Loss: 0.609809 \tValidation Loss: 38.723305\n",
      "Epoch: 62, Loss: 0.6096075890032021\n",
      "Epoch: 62 \tTraining Loss: 0.609608 \tValidation Loss: 38.669545\n",
      "Epoch: 63, Loss: 0.6099514087309709\n",
      "Epoch: 63 \tTraining Loss: 0.609951 \tValidation Loss: 38.799704\n",
      "Epoch: 64, Loss: 0.6098846987158328\n",
      "Epoch: 64 \tTraining Loss: 0.609885 \tValidation Loss: 38.365853\n",
      "Epoch: 65, Loss: 0.609818336394456\n",
      "Epoch: 65 \tTraining Loss: 0.609818 \tValidation Loss: 38.421035\n",
      "Epoch: 66, Loss: 0.6094943477629541\n",
      "Epoch: 66 \tTraining Loss: 0.609494 \tValidation Loss: 38.287549\n",
      "Validation loss decreased (38.306067 --> 38.287549).                     Saving model ...\n",
      "Epoch: 67, Loss: 0.6090553124342952\n",
      "Epoch: 67 \tTraining Loss: 0.609055 \tValidation Loss: 38.355480\n",
      "Epoch: 68, Loss: 0.609186488765854\n",
      "Epoch: 68 \tTraining Loss: 0.609186 \tValidation Loss: 38.455106\n",
      "Epoch: 69, Loss: 0.6103147324006837\n",
      "Epoch: 69 \tTraining Loss: 0.610315 \tValidation Loss: 38.599671\n",
      "Epoch: 70, Loss: 0.6091678909756042\n",
      "Epoch: 70 \tTraining Loss: 0.609168 \tValidation Loss: 38.401009\n",
      "Epoch: 71, Loss: 0.6118264488942988\n",
      "Epoch: 71 \tTraining Loss: 0.611826 \tValidation Loss: 38.510745\n",
      "Epoch: 72, Loss: 0.6086590134345733\n",
      "Epoch: 72 \tTraining Loss: 0.608659 \tValidation Loss: 38.520880\n",
      "Epoch: 73, Loss: 0.6079735061875334\n",
      "Epoch: 73 \tTraining Loss: 0.607974 \tValidation Loss: 38.462683\n",
      "Epoch: 74, Loss: 0.6073516741529241\n",
      "Epoch: 74 \tTraining Loss: 0.607352 \tValidation Loss: 38.372379\n",
      "Epoch: 75, Loss: 0.6098250401046899\n",
      "Epoch: 75 \tTraining Loss: 0.609825 \tValidation Loss: 38.612923\n",
      "Epoch: 76, Loss: 0.6074140498767028\n",
      "Epoch: 76 \tTraining Loss: 0.607414 \tValidation Loss: 38.288823\n",
      "Epoch: 77, Loss: 0.6082312008953309\n",
      "Epoch: 77 \tTraining Loss: 0.608231 \tValidation Loss: 38.408491\n",
      "Epoch: 78, Loss: 0.6094493673996882\n",
      "Epoch: 78 \tTraining Loss: 0.609449 \tValidation Loss: 38.554860\n",
      "Epoch: 79, Loss: 0.6073268678139996\n",
      "Epoch: 79 \tTraining Loss: 0.607327 \tValidation Loss: 38.481363\n",
      "Epoch: 80, Loss: 0.6077157908060529\n",
      "Epoch: 80 \tTraining Loss: 0.607716 \tValidation Loss: 38.336532\n",
      "Saving the model.\n",
      "CPU times: user 1min 28s, sys: 978 ms, total: 1min 29s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "## TODO: Define an optimizer and loss function for training\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "\n",
    "train(model, train_loader, validation_loader, 80, optimizer, criterion, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea6c092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2228a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16])\n"
     ]
    }
   ],
   "source": [
    "for data, labels in test_loader:\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "92599f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.592\n",
      "Test accuracy: 0.697\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "#h = net.init_hidden(batch_size)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for data, targets in test_loader:\n",
    "\n",
    "    # get output of SimpleNet\n",
    "    output = model(data)\n",
    "    # calculate loss and perform backprop\n",
    "    test_loss = criterion(output, targets)\n",
    "\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(targets.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e692a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881ec81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
